# ğŸ› ï¸ Legacy Job Orchestration Modernization with Apache Airflow and AWS

This project demonstrates how to modernize legacy Windows-scheduled ETL scripts into robust, scalable Apache Airflow DAGs â€” deployed using Docker and integrated with AWS services like S3 and Redshift.

---

## ğŸš€ Features
- Migrate `.bat`/`.py` scripts to Airflow DAGs
- ETL pipeline with extract-transform-load steps
- Data ingestion from public API
- AWS S3 integration
- Dockerized Airflow environment (webserver + scheduler)
- Retry logic, logging, and alerting scaffolding
- Fully documented for learning and reusability

---

## ğŸ“ Folder Structure
```
airflow-aws-modernization/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ legacy_to_airflow_dag.py         # Your main DAG file (with comments)
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml               # Airflow deployment config
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ extract_transform_load.py        # Reusable ETL logic (optional)
â”œâ”€â”€ redshift/
â”‚   â””â”€â”€ create_tables.sql                # SQL DDL to prepare Redshift table
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ airflow_aws_architecture.png     # Architecture diagram (to be created)
â”œâ”€â”€ LICENSE                              # MIT license file
â”œâ”€â”€ README.md                            # Full project documentation
â”œâ”€â”€ .gitignore                           # With Python, Airflow, Docker exclusions
â””â”€â”€ .env.example                         # Placeholder for AWS credentials (not tracked)
```

## ğŸ§± Tech Stack
- Apache Airflow 2.7.1 (Dockerized)
- Python 3.9
- PostgreSQL (Airflow backend)
- AWS S3 (data storage)
- Pandas + Requests + Boto3

---

## ğŸ”§ How to Run

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/airflow-aws-modernization.git
cd airflow-aws-modernization
```

### 2. Set up environment variables
Copy the sample environment file:
```bash
cp .env.example .env
```
Add your AWS credentials to `.env`.

### 3. Start Airflow with Docker Compose
```bash
cd docker
docker-compose up -d
```

### 4. Access the Airflow UI
Go to [http://localhost:8080](http://localhost:8080) and enable the `legacy_to_airflow_dag`.

---

## ğŸ—‚ï¸ DAG Tasks Overview
- **Extract**: Pull product data from a public API
- **Transform**: Add derived fields (e.g., tax-calculated price)
- **Load**: Upload CSV to AWS S3 bucket

---

## ğŸ“Š Architecture
> Diagram available at: `docs/airflow_aws_architecture.png` (to be added)

---

## ğŸ§  About the Author
Built with â¤ï¸ by **Bita Ashoori** â€” Data Engineer & Automation Enthusiast

---

## ğŸ“„ License
Licensed under the [MIT License](LICENSE).

